{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.pipeline import Pipeline\n",
        "from scipy.sparse import hstack"
      ],
      "metadata": {
        "id": "bVFQetz9Mg9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load cleaned datasets\n",
        "trainingSet = pd.read_csv('/content/train_fixed_cleaned.csv')\n",
        "testingSet = pd.read_csv('/content/test.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "QTjkxhZxMiQH",
        "outputId": "428d9e1c-8d5a-447c-cc3b-04d5bc2dd5b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "Error tokenizing data. C error: EOF inside string starting at row 93678",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-cf9098f3eae3>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load cleaned datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainingSet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/train_fixed_cleaned.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtestingSet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1921\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1922\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1924\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1925\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: EOF inside string starting at row 93678"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/train_fixed_cleaned.csv'\n",
        "\n",
        "# Check for rows with an odd number of quotes\n",
        "with open(file_path, 'r', encoding='utf-8') as f:\n",
        "    for i, line in enumerate(f):\n",
        "        if line.count('\"') % 2 != 0:  # Check for odd number of quotes\n",
        "            print(f\"Issue with quotes on line {i + 1}: {line}\")\n",
        "            break  # Stop at first issue to investigate\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWhS0H5tNbWb",
        "outputId": "ce2fe844-24e8-4998-9219-5d28b113fcc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Issue with quotes on line 269827: 1044134,B000M8NMVE,A34MWK16CNJAVN,15,16,1190937600,Danger Is His Business,\"I remember watching this show before school. Only in B&W;, (okay, so I'm old) and never missed an episode. Now, finally, I can re-live those early mornings in color, and in pristine clarity. I have to say, as soon as the menu came up with the theme song, my old grey cells kicked in, and I had a smile from ear to ear.Like \"\"The Archies\"\", \"\"Groovie Goolies\"\" and \"\"Top Cat\"\", the REAL openings are there, captured in their FULL format. Unlike \"\"Magilla Gorila\"\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "input_path = '/content/train_fixed_cleaned.csv'\n",
        "output_path = '/content/train_fixed_cleaned_corrected.csv'\n",
        "\n",
        "# Determine the expected number of columns based on the header row\n",
        "with open(input_path, 'r', encoding='utf-8') as infile:\n",
        "    reader = csv.reader(infile)\n",
        "    header = next(reader)\n",
        "    expected_columns = len(header)\n",
        "\n",
        "# Process the file, fixing problematic rows\n",
        "with open(input_path, 'r', encoding='utf-8') as infile, open(output_path, 'w', newline='', encoding='utf-8') as outfile:\n",
        "    reader = csv.reader(infile)\n",
        "    writer = csv.writer(outfile)\n",
        "\n",
        "    writer.writerow(header)  # Write the header to the output file\n",
        "\n",
        "    for i, row in enumerate(reader, start=2):  # Start at line 2 to account for header\n",
        "        # If row has fewer or more columns than expected, attempt to fix it\n",
        "        if len(row) != expected_columns:\n",
        "            fixed_row = []\n",
        "\n",
        "            for field in row:\n",
        "                # Correct fields with unbalanced quotes\n",
        "                if field.count('\"') % 2 != 0:\n",
        "                    field = field.replace('\"', '')  # Remove stray quotes\n",
        "                fixed_row.append(field)\n",
        "\n",
        "            # Flatten the row to the expected length\n",
        "            fixed_row = fixed_row[:expected_columns] + [''] * (expected_columns - len(fixed_row))\n",
        "            writer.writerow(fixed_row)\n",
        "        else:\n",
        "            writer.writerow(row)\n"
      ],
      "metadata": {
        "id": "98a002VMNwUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load cleaned file\n",
        "trainingSet = pd.read_csv('/content/train_fixed_cleaned_corrected.csv')\n",
        "print(\"File loaded successfully with corrected rows.\")\n"
      ],
      "metadata": {
        "id": "mbiaxxvoNy-n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6ac2380-9c67-426d-f183-3e1ebf5f3705"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File loaded successfully with corrected rows.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-9e64ff4d9e12>:4: DtypeWarning: Columns (0,3,4,5,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  trainingSet = pd.read_csv('/content/train_fixed_cleaned_corrected.csv')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testingSet = pd.read_csv('/content/test.csv')"
      ],
      "metadata": {
        "id": "y1pXx7FFOWPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure that HelpfulnessNumerator and HelpfulnessDenominator are numeric\n",
        "trainingSet['HelpfulnessNumerator'] = pd.to_numeric(trainingSet['HelpfulnessNumerator'], errors='coerce').fillna(0)\n",
        "trainingSet['HelpfulnessDenominator'] = pd.to_numeric(trainingSet['HelpfulnessDenominator'], errors='coerce').fillna(1)\n",
        "\n",
        "# Drop rows where 'Score' is missing\n",
        "trainingSet.dropna(subset=['Score'], inplace=True)\n",
        "\n",
        "# Feature engineering on training data\n",
        "trainingSet['HelpfulnessRatio'] = trainingSet['HelpfulnessNumerator'] / trainingSet['HelpfulnessDenominator']\n",
        "trainingSet['ReviewLength'] = trainingSet['Text'].apply(lambda x: len(str(x)))\n"
      ],
      "metadata": {
        "id": "YQCTEfx5NB66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Step 1: Create the Document-Term Matrix\n",
        "count_vect = CountVectorizer(max_features=3000)\n",
        "count_matrix = count_vect.fit_transform(trainingSet['Text'].fillna(\"\"))\n",
        "\n",
        "# Step 2: Apply Truncated SVD for dimensionality reduction\n",
        "svd = TruncatedSVD(n_components=100, random_state=42)  # Adjust components based on memory constraints\n",
        "text_features = svd.fit_transform(count_matrix)\n"
      ],
      "metadata": {
        "id": "4EcLGOoCNDSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Clustering on reduced features\n",
        "kmeans = KMeans(n_clusters=5, random_state=42)\n",
        "trainingSet['ClusterLabel'] = kmeans.fit_predict(text_features)"
      ],
      "metadata": {
        "id": "5LMgdOiRNGmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.sparse import hstack\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "# Ensure the 'Score' column is an integer type\n",
        "trainingSet['Score'] = pd.to_numeric(trainingSet['Score'], errors='coerce').fillna(0).astype(int)\n",
        "\n",
        "# Prepare numerical features and shift to non-negative values\n",
        "X_numerical = trainingSet[['HelpfulnessRatio', 'ReviewLength', 'ClusterLabel']].fillna(0)\n",
        "\n",
        "# Apply MinMaxScaler to ensure non-negative values for numerical features\n",
        "scaler_numerical = MinMaxScaler()\n",
        "X_numerical_scaled = scaler_numerical.fit_transform(X_numerical)\n",
        "\n",
        "# Apply MinMaxScaler to ensure non-negative values for text features\n",
        "scaler_text = MinMaxScaler()\n",
        "text_features_scaled = scaler_text.fit_transform(text_features)\n",
        "\n",
        "# Combine scaled numerical and text features\n",
        "X = hstack([csr_matrix(X_numerical_scaled), csr_matrix(text_features_scaled)])\n",
        "y = trainingSet['Score']\n",
        "\n",
        "# Split data into train and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model training with Naive Bayes\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "y_pred_nb = nb_model.predict(X_val)\n",
        "\n",
        "# Validation Results\n",
        "print(\"Naive Bayes Accuracy:\", accuracy_score(y_val, y_pred_nb))\n",
        "print(\"Naive Bayes Classification Report:\\n\", classification_report(y_val, y_pred_nb))\n"
      ],
      "metadata": {
        "id": "T23eZDcAM7zi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d69ce950-1fdf-4ad1-c690-c18773fc172f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes Accuracy: 0.5328886451163916\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.00      0.00      0.00     10474\n",
            "           2       0.00      0.00      0.00     10392\n",
            "           3       0.00      0.00      0.00     20076\n",
            "           4       0.00      0.00      0.00     38260\n",
            "           5       0.53      1.00      0.70     90355\n",
            "\n",
            "    accuracy                           0.53    169557\n",
            "   macro avg       0.11      0.20      0.14    169557\n",
            "weighted avg       0.28      0.53      0.37    169557\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation Results\n",
        "print(\"Naive Bayes Accuracy:\", accuracy_score(y_val, y_pred_nb))\n",
        "print(\"Naive Bayes Classification Report:\\n\", classification_report(y_val, y_pred_nb))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54W35yKQSGut",
        "outputId": "49589a63-a0c7-4186-bd51-3c71ee389608"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes Accuracy: 0.5328886451163916\n",
            "Naive Bayes Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.00      0.00      0.00     10474\n",
            "           2       0.00      0.00      0.00     10392\n",
            "           3       0.00      0.00      0.00     20076\n",
            "           4       0.00      0.00      0.00     38260\n",
            "           5       0.53      1.00      0.70     90355\n",
            "\n",
            "    accuracy                           0.53    169557\n",
            "   macro avg       0.11      0.20      0.14    169557\n",
            "weighted avg       0.28      0.53      0.37    169557\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge test set with necessary columns from training data\n",
        "test_data = testingSet.merge(trainingSet[['Id', 'HelpfulnessRatio', 'ReviewLength', 'ClusterLabel', 'Text']], on='Id', how='left')\n",
        "\n",
        "# Fill or recompute missing features in test set\n",
        "test_data['HelpfulnessRatio'] = test_data['HelpfulnessRatio'].fillna(0)\n",
        "test_data['ReviewLength'] = test_data['Text'].apply(lambda x: len(str(x)) if pd.notnull(x) else 0)\n",
        "test_data['Text'] = test_data['Text'].fillna(\"\")\n",
        "\n",
        "# Transform text features in test data\n",
        "test_count_matrix = count_vect.transform(test_data['Text'])\n",
        "test_text_features = svd.transform(test_count_matrix)\n",
        "\n",
        "# Prepare test features\n",
        "X_test_numerical = csr_matrix(test_data[['HelpfulnessRatio', 'ReviewLength', 'ClusterLabel']].fillna(0).values)\n",
        "X_test = hstack([X_test_numerical, csr_matrix(test_text_features)])\n",
        "\n",
        "# Predict on the test set\n",
        "test_predictions = nb_model.predict(X_test)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "W5EHxH--P3Ka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the final submission\n",
        "testingSet['Score'] = test_predictions\n",
        "submission = testingSet[['Id', 'Score']]\n",
        "submission.to_csv(\"/content/submission_7.csv\", index=False)\n",
        "print(\"Submission file saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuG4qNINRfcN",
        "outputId": "377a8d02-ce56-4bef-a180-a0f0ac580d18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submission file saved.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}